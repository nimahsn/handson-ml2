{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 7: Ensemble Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Voting Classifiers </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we have a few classifiers like a Logistic Regression, an SVM classifier, a Random Forest classifier, a K-Nearest Neighbors classifier and etc, each achieving an acceptable accuracy, say 80%. A very simple way to create an even better classifier is to aggregate the predictions of each classifier and for each input, predict the class that gets the most votes. This majority-vote classifier is called a <i><b>hard voting classifier</b></i>. This new classifier often achieves a higher accuracy than those in the ensemle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods like this one work best when the predictors in the ensemble are as independent from one another as possible. When all the predictors are trained on the same training set, they are not purely independent. One way to achieve rather diverse classifiers to use in ensemble is to train them using very different algorithms. This way, the chance that they will make different types of errors increases, improving the ensemble's accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A voting classfier can be imlpemented using scikit learn <code>VotingClassifier</code> class from <code>ensemble</code> package. To use hard voting method, <code>'hard'</code> argument must be used for <code>voting</code> parameter.\n",
    "<br>\n",
    "<br>\n",
    "<code>log_clf = LogisticRegression()\n",
    "rnd_clf = RandomForestClassifier()\n",
    "svm_clf = SVC()\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "voting_clf.fit(X_train, y_train)\n",
    "</code>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If all the classifiers in ensemble are capable of computing a probability for each class (i.e. they have <code>predict_proba()</code> method), then we can tell scikit-learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called <i><b> soft voting</b></i>. It can achieve higher scores than hard voting since it gives more weight to highly confident votes. <code>voting=\"hard\"</code> must be replaced with <code>voting=\"soft\"</code> to use this method. Keep in mind that SVC doesn't compute probabalities by default, so we need to set its <code>probability</code> hyperparameter to <code>True</code>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Bagging and Pasting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to get a diverse set of classifiers is to use the same training algorithm for all predictors and train them on defferent random subsets of the training set. When sampling is performed with replacement, it's called <i><b>bagging</b></i> (short for bootstrap aggregating). When sampling is done without replacement, it's called <i><b>pasting</b></i>. <br>\n",
    "Both Bagging and Pasting allow taining instances to be sampled several time across multiple predictors, but only bagging allows training instances to be sampled several times for the same predictor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once all predictors are trained, the ensemble can make a prediction for a new instance by aggregating the predictions of all predictors. For classification, the aggreagation function is typically <i>statistical mode</i>. For regression, ensemble computes average of all predictions. Each predictor has more bias than if it was trained on the whole training set, but aggreagtion reduces both bias and variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bagging and Pasting can be performed with scikit-learn API through <code>BaggingClassifier</code> class from ensemble package. This will perform Bagging bu default, but if you want to use pasting, set <code>bootsrtap</code> parameter to <code>false</code>.<br><br>\n",
    "<code>bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1)\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)</code><br>\n",
    "- Note that BaggingClassifier performs soft voting by default if the base predictor have <code>predict_proba()</code> method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bootstrapping introduces a bit more diversity in the subsets that each predictor is\n",
    "trained on, so bagging ends up with a slightly higher bias than pasting; but the extra\n",
    "diversity also means that the predictors end up being less correlated, so the ensemble’s\n",
    "variance is reduced. Overall, bagging often results in better models, which explains why\n",
    "it is generally preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Out-of-Bag Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Out of Bag evaluation is a way to validate a bagging ensemble model. In a bagging model, each base predictor is trained on approximately 67% of the training instances (in limit of large training set size). The remaining 37% of training instances that are not sampled are called <i>out-of-bag</i> instances. Note that they are not the same 37% for all predictors. Since a predictor never sees the oob instances during training, it can be evaluated on these instances, without the need for a seperate validation set.The ensemble itself can be evaluated by averaging all the oob scores. Also note that oob evaluation is different from cross-validation and it can be useful when the training set is not big enough to put aside some instances for CV. For more info head to: https://towardsdatascience.com/what-is-out-of-bag-oob-score-in-random-forest-a7fa23d710 <br>\n",
    "In scikit learn, oob score is computed if you set the <code>oob_score</code> parameter to <code>True</code> when creating the BaggingClassifier object. The resulting evaluation can be accsessed through the <code>oob_score_</code> variable in bagging model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Random Patches and Random Subspaces</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can sample features too, just like what we did for training instances. Doing so can be useful when we are dealing with high-dimensional inputs like images. If we sample both instances and features, the moethod is called <i>Random Patches</i>. If we keep all the training instances but sample features, it's called <i>Random Subspaces</i> method. These two methods can be implemented with the same class from scikit-learn by setting two <code>max_features</code> and <code>bootstrap_features</code> parameters.<br>\n",
    "Sampling features results in even more predictor diversity, trading a bit more bias for a\n",
    "lower variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Random Forests</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest is an ensemble of decision trees, generally trained via bagging method, typically with <code>max_samples</code> set to the size of the training set. Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, we can use the RandomForestClassifier class, which is more convenient and optimized for Decision Trees.<br>\n",
    "The Random Forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra-Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are growing a tree in a Random Forest, at each node only a random subset of\n",
    "the features is considered for splitting (as discussed earlier). It is possible to make trees\n",
    "even more random by also using random thresholds for each feature rather than\n",
    "searching for the best possible thresholds (like regular Decision Trees do).<br>A forest of such extremely random trees is called an Extremely Randomized Trees\n",
    "ensemble (or Extra-Trees for short). Once again, this technique trades more bias for a\n",
    "lower variance. <br>You can create an Extra-Trees classifier using Scikit-Learn’s ExtraTreesClassifier\n",
    "class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn measures a feature’s importance by\n",
    "looking at how much the tree nodes that use that feature reduce impurity on average\n",
    "(across all trees in the forest). More precisely, it is a weighted average, where each\n",
    "node’s weight is equal to the number of training samples that are associated with it.<br>\n",
    "Scikit-Learn computes this score automatically for each feature after training, then it\n",
    "scales the results so that the sum of all importances is equal to 1. You can access the\n",
    "result using the feature_importances_ variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Boosting</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct to correct its predecessor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaBoosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoosting is a boosting method which a new predictor pays more attention to the training instances that are underfitted by it's predecesor. This results in new predictors focusing more and more on the hard cases. For eaxample, an AdaBoost classifier first trains a base classifier like a decision tree. The algorithm then increases the relative weight of misclassified training instances. Then it trains a new classifier using the updated weights, and so on.<br>\n",
    "In AdaBoost each instance weight $w^i$ is initially set to $\\frac{1}{m}$. After training the first predictor, it's weighted error rate $r_1$ is computed on the training set:\n",
    "\n",
    "$$r_{j}=\\frac{\\sum_{i=1 \\atop{\\hat{y}_{j}^{(i)} \\neq y^{(i)}}}^{m} w^{(i)}}{\\sum_{i=1}^{m} w^{(i)}} \\text{where } \\hat{y}_{j}^{(i)} \\text { is the } j^{\\text {th }} \\text{ predictor's prediction for the } i^{\\text {th }} \\text{ instance.}$$\n",
    "\n",
    "The predictor's weight $\\alpha_j$  is the computed using the following equation, where $\\eta$ is the learning rate hyperparameter. The more accurate the predictor is, the higher its weight will be.\n",
    "\n",
    "$$\n",
    "\\alpha_j = \\eta\\log{\\frac{1-r_j}{r_j}}\n",
    "$$\n",
    "\n",
    "Next, instance weights are updated, using this equation, which boosts the weights of the misclassified samlpes:\n",
    "\n",
    "$$\\text{for } i = 1,2,... ,m\\newline\n",
    "w^{(i)}\\leftarrow \n",
    "\\left\\{\\begin{matrix}\n",
    " w^{(i)}&\\text{if } \\hat{y}_j^{(i)}=y^{(i)} \\\\ \n",
    " w^{(i)}\\exp{(\\alpha_j)}& \\text{if }\\hat{y}_j^{(i)} \\neq {y^{(i)}}\n",
    "\\end{matrix}\\right.\n",
    "$$\n",
    "Then all the weights are normalized (i.e., divided by $\\sum_{i=1}^{m}w^{(i)}$).\n",
    "<br>\n",
    "Finally, a new predictor is trained using the new weigths, and the whole process is repeated. The algorithm stops when the desired number of predictors is reached, or when a perfect predictor is found.<br>\n",
    "To make predictions, AdaBoost simply computes the predictions of all the predictors\n",
    "and weighs them using the predictor weights $\\alpha_j$.\n",
    "$$\\hat{y}({X}) = \\operatorname*{arg\\,max}_k \\sum_{j=1 \\atop {\\hat{y}_j(x)=k}}^N \\alpha_j \\text{where } N \\text{is the number of predictors.}\n",
    "$$\n",
    "<br>\n",
    "AdaBoost is availbale in Scikit-Learn ensemble package. It uses a multiclass version of Adaboost called <i>SAMME</i>. Also if the base predictor can estimate probabilities, Scikit can use <i>SAMME.R</i> (R for \"Real\") which depends on class probabilities rather than predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosting, like AdaBoost, works by sequentially adding predictors to an ensemble, each one correcting it's predecessor. However, instead of tweaking instance weights at each iteration, this method tries to fit the new predictor to the residual errors made by the previous predictor. It first trains a base predictor on the training set, then predicts the training set with the same predictor and measures the absolute errors made by this predictor for each instance. The second predictor is then trained with these errors as target values. Third predctor is again trained on the second predictor errors and so on. The algorithm stops when it reaches the maximum number of estimators or it can be controlled with early stopping. \n",
    "<br>\n",
    "Gradient boosting can be used in regression and classification tasks. When performing regression with decision trees and gradient boosting, the algorithm is called <i>Gradient Tree Boosting</i>, or <i>Gradient Boosted Regression Trees</i> (GBRT). Gradient Boosting regression and classification is available in scikit-learn with trees as base predictors. It has hyperparameters to control the growth of decision trees (e.g., max_depth), as well as hyperparameters to control the ensemble, such as number of trees (<code>n_estimators</code>).<br>The GradientBoostingRegressor class also supports a subsample hyperparameter,\n",
    "which specifies the fraction of training instances to be used for training each tree. For\n",
    "example, if subsample=0.25 , then each tree is trained on 25% of the training instances,\n",
    "selected randomly. As you can probably guess by now, this technique trades a higher\n",
    "bias for a lower variance. It also speeds up training considerably. This is called\n",
    "Stochastic Gradient Boosting.<br>\n",
    "It is worth noting that an optimized implementation of Gradient Boosting is available in\n",
    "the popular Python library XGBoost, which stands for Extreme Gradient Boosting. This\n",
    "package was initially developed by Tianqi Chen as part of the Distributed (Deep)\n",
    "Machine Learning Community (DMLC), and it aims to be extremely fast, scalable, and\n",
    "portable.\n",
    "\n",
    "\n",
    "Check out the book and it's notebooks for code examples and more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=blue>Stacking</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble methods we discussed in the first of chapter like voting classfiers, usually use trivial aggregation functions to aggregate all the predictions. Stacking is based on a simple idea: instead of using trivial functions, why don't we train a model to perform this aggregateion? In such an ensemble, there is a layer of all the base predictors and the aggregation function which is called a <i>blender</i>, or a <i>meta laerner</i> is in another layer.<br><br>\n",
    "To train a blender, a commom approach is to use a hold-out set. First, the training set is split into two subsets. One used two train the predictors in the first layer. Next, the first layer's predictors are used to make predictions on the second (held-out) set. For each instance in the hold-out set and each predictor, there is a predicted value that can be used to create a new training set with these predicted values as features and same target values as in the main train set. The blender is trained on this new training set, so it learns to predict the target value, given the first layer’s predictions.<br><br>\n",
    "It is possible to train several different blenders this way, to get a whole layer of blenders with a third layer  which aggregates blenders' predictions. The trick here is to split the training set into three subsets, each for training one layer. Once this is done, we can make a prediction for a new instance by going through each layer sequentially.\n",
    "<br><br>\n",
    "Scikit-Learn doesn't support stacking but it is not too hard to implement one. Alternatively, we can use some other implementations such as <i>brew</i>."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
