{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5 - Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <i>Support Vector Machine</i> is a powerful and versatile Machine Learning model, capable of performin linear and non-linear classification, regression and even outlier detection. They are particularly well suited for classification of complex small- or medium-sized datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <font color=blue>Linear SVM Classification</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "A Linear SVC tries to seperate 2 classes with a decision boundary which stays as far away from the closet training instances as possible. We can think of an SVM classifier as fitting the weidest possible street between the classes. This is called <i>large margin</i> classification. This boundary or street is not affected by the instances off the street; it's fully determined (or supported) by the instances located on the edge of the street. These supportting instances are called <i>support vectors<i>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "#### <font color=red>Warning</font>\n",
    "SVMs are highly sensitive to the feature scales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Soft Margin Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If we strictly impose that all training instances must be off the street and on the correct side, this is called <i>hard margin classification</i>. Hard margin only works when two classes are linearly seperable. This will result in a worse issue with hard margin SVC: an outlier can have a high impact on the decision boundary and might even make it impossible to train the model.<br>\n",
    "To avoid these issues we should develop a new model which keeps a good balance between keeping the street as large as possible and limiting number of violations (instances that end up in the middle of the street or even on the wrong side). This is called <i>soft margin classification</i>.<br>\n",
    "When creating an SVM model using Scikit-Learn, we specify a number of hyperparameters. <code>C</code> is one those hyperparameters. If we set it to a low value, then we end up with a model with wider street but more violations. On the other hand, increasing <code>C</code>, decreases the number of violations and make the street narrower. Generally, violation are bad, but a model with a wider street might generalize much better. Thus, if the model is overfitting, we can try decreasing <code>C</code>.<br><br>\n",
    "To create a linear svm classifier, we can use <code>LinearSVC</code> class with <i>hinge loss</i> function. Alternatively, we can create one, using <code>SVC</code> class with <code>kernel=\"linear\"</code>.<br>\n",
    "Instead of these two, we could use <code>SGDClassifier</code> with <code>loss=\"hine\"</code> and <code>alpha=1/(m*C))</code>. This will converge slower than LinearSVC but it supports out-of-core learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <font color=blue> Nonlinear SVM Classification </font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Although linear SVMs are so capable and powerful, some datesets are not even close to being linearly seperable. Adding more polynomial features, as we did in chapter 4, might result in a linearly seperable data set. This can be achieved in a similar manner to what we did in chapter 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Adding polynomial features, as descrive above, is easy and accessible in many Machine Learning algorithms. However, adding many features can highly imapact algorithm performance and make it so slow.<br>\n",
    "When using SVMs you can apply an almost miraculous mathematical technique called the kernel trick (explained in a moment). The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with very high-degree polynomials, without actually having to add them. This trick is implemented by SVC class. The <code>kernel</code> hyperparameter must be set to <code>\"poly\"</code> and <code>degree</code> must be set to the wanted polnomial degree. The hyperparameter <code>coef0</code> controls how much the model is affected by the high degree polynomials versus low degree polynomials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Similarity Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Another technique to tackle nonlinear problems is to add features computed using a <i>similarity function</i>, which measures how much each instace resembles a particular <i>landmark</i>. Smilarity function can be defined to be the Gaussian <i>radial basis function</i> (RBF):\n",
    "$$\\phi_\\gamma (x, \\ell) = \\exp\\left(-\\gamma\\|x-\\ell\\|^2\\right)$$\n",
    "This is a bell-shaped function varying from 0 (far away from the landmark) to 1 (at the landmark). Given the landmark $\\ell$, it will compute a similarity value for each input $x$. So these landmarks become the new features of the dataset and similarities of each instance to landmarks, computed by the similarity function, become the new values for each input. This method, like polynomial features, can make the dataset linearly seperable. Landmarks can be simply chosen to be at the location of each and every instance in the dataset. Doing that makes many dimension which probably makes the dataset liearly seperable. However this method transforms an $m\\times n$ dataset to an $m\\times m$ dataset. If the dataset is very large, you end up with an equally large number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Gaussian RBF Kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Just like polynimial features, this similarity features can be useful with any Machine Learning algorithm., but it maybe comutationally expensive. Once again the kernel trick does its SVM magick, making it possible to obtain a similar result as if you had added many similarity features.<br>\n",
    "To use this method with Scikit-Learn, we must set <code>kernel=\"rbf\"</code> in SVC. We can control shape of the rbf function with <code>gamma</code> hyperparameter. Increasing it makes the bell-shaped curve narrower. As a result, each instanceâ€™s range of influence is smaller: the decision boundary ends up being more irregular, wiggling around individual instances. Conversely, a small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. So $\\gamma$ acts like a regularization hyperparameter: if your model is overfitting, you should reduce it; if it is underfitting, you should increase it (similar to the C hyperparameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- There are other kernels available too, but they are used more rarely and some of them have special use cases such as classifying text documents or DNA sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- The tolerance hyperparamter, $\\epsilon$ (called <code>tol</code> in Scikit-Leran) controls the precision of an SVM model. The algorithm takes longer if you require very high precision."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## <font color=blue>SVM Regression</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To use SVMs\n",
    "for regression instead of classification, the trick is to reverse the objective: instead of\n",
    "trying to fit the largest possible street between two classes while limiting margin\n",
    "violations, SVM Regression tries to fit as many instances as possible on the street while\n",
    "limiting margin violations (i.e., instances off the street). The width of the street is\n",
    "controlled by a hyperparameter, $\\epsilon$.\n",
    "<br>\n",
    "Tricks and methods used described for classification can also be used with SVM Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
