{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Training  Models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a linear model makes a predictipon by simply computing a weighted sum of the input features, plus a constant called the bias term:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}=\\theta_{0}+\\theta_{1} x_{1}+\\theta_{2} x_{2}+\\cdots+\\theta_{n} x_{n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\hat{y}=h_{\\theta}(\\mathbf{x})=\\boldsymbol{\\theta} \\cdot \\mathbf{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\widehat{\\boldsymbol{\\theta}}=\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)^{-1} \\mathbf{X}^{\\top} \\mathbf{y}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This equation gives us the weights directly\n",
    "- pseudo inverse can be used instead of inverse, as $\\left(\\mathbf{X}^{\\top} \\mathbf{X}\\right)$ might be singular, as result of redundant features or not enough instances\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit learn LinearRegression class uses normal equations and is based on scipy.linalg.lstsq() which computes pseudo inverse using SVD."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "computing inverse of this matrix has a computational complexity of $O(n^2.4)$ to $O(n^3)$ depending on the implementation. scikit learn approach's complexity is about $O(n^2)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "normal equation and svd method get very slow as numver of features ($n$) grows. On the positive side they are linear regarding to the number of instances."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Descent algorithm tweaks parameters iteratively in order to minimize a cost function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch gradient descent computes the partial derivative if the cost function with regard to parameter $\\theta_j$ :\n",
    "$$\\frac{\\partial}{\\partial \\theta_{j}} \\operatorname{MSE}(\\boldsymbol{\\theta})=\\frac{2}{m} \\sum_{i=1}^{m}\\left(\\boldsymbol{\\theta}^{\\top} \\mathbf{x}^{(i)}-y^{(i)}\\right) x_{j}^{(i)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can re write this in the form below to compute all the derivatives in one go.The gradient vector contains all the derivatives of the cost function:\n",
    "$$\\nabla_{\\theta} \\operatorname{MSE}(\\boldsymbol{\\theta})=\\left(\\begin{array}{c}\n",
    "\\frac{\\partial}{\\partial \\theta_{0}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{1}} \\operatorname{MSE}(\\boldsymbol{\\theta}) \\\\\n",
    "\\vdots \\\\\n",
    "\\frac{\\partial}{\\partial \\theta_{\\mathrm{n}}} \\operatorname{MSE}(\\boldsymbol{\\theta})\n",
    "\\end{array}\\right)=\\frac{2}{m} \\mathbf{X}^{\\top}(\\mathbf{X} \\boldsymbol{\\theta}-\\mathbf{y})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This method computes the derivatives over the whole training set $\\mathbf{X}$ at each step. this is why it's called Batch Gradient Descent. As a result it gets slow on very large training sets. However, Gradient Descent scales well with the number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the gradient vector, we can calculate new weights:\n",
    "$$\\boldsymbol{\\theta}^{(\\text {next step })}=\\boldsymbol{\\theta}-\\eta \\nabla_{\\boldsymbol{\\theta}} \\operatorname{MSE}(\\boldsymbol{\\theta})$$ where $\\eta$ is the learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Descent, unlike Batch Gradient Descent, picks a random instance in training set at every training step and computes gradients based on that single instance. Over time it will get close to the minimum, but once it gets there it will continue to bounce around, never settling down. So once the algorithm stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pros:\n",
    "- faster convergence\n",
    "- more likely to find global minimum\n",
    "- can be implemented as an out-of-core algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the randomness is good for escaping from local optima by jumping over, it's bad as it can never settle at the minimum. one solution to this problem is to gradually reduce the learning rate. the learning rate should not be reduced too quickly or too slowly. The function that determines the learning rate at each iteration is called the learning schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In SGD implementation, epochs are used instead of iterations. While BGD calculates over the whole data set in every iteration, SGD calculates derivatives over every instance separately in every epoch; so it goes through the training set on each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using SGD, the training instances must be independent and identically disturbed to ensure that the parameters get pulled toward the global optimum. A simple way to ensure this is to shuffle the instances\n",
    "during training (e.g., pick each instance randomly, or shuffle the training set at the\n",
    "beginning of each epoch). If you do not shuffle the instances—for example, if the\n",
    "instances are sorted by label—then SGD will start by optimizing for one label, then the\n",
    "next, and so on, and it will not settle close to the global minimum.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-Learn has implemented SGD in SGDRegressor class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-Batch GD, instead of training on whole training set as in BGD or training on one instance as in SGD on each step, computes gradients on a small random set of instances. Mini-Batch GD benefits from hardware optimization of matrix operations, especially when using GPUs. It will end up walking around a bit closer to the minimum than\n",
    "Stochastic GD—but it may be harder for it to escape from local minima."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{array}{llllll}\n",
    "\\hline \\text { Algorithm } & \\begin{array}{l}\n",
    "\\text { Large } \\\\\n",
    "m\n",
    "\\end{array} & \\begin{array}{l}\n",
    "\\text { Out-of-core } \\\\\n",
    "\\text { support }\n",
    "\\end{array} & \\begin{array}{l}\n",
    "\\text { Large } \\\\\n",
    "\\boldsymbol{n}\n",
    "\\end{array} & \\text { Hyperparams } & \\begin{array}{l}\n",
    "\\text { Scaling } \\\\\n",
    "\\text { required }\n",
    "\\end{array} & \\text { Scikit-Learn } \\\\\n",
    "\\hline \\begin{array}{l}\n",
    "\\text { Normal } \\\\\n",
    "\\text { Equation }\n",
    "\\end{array} & \\text { Fast } & \\text { No } & \\text { Slow } & 0 & \\text { No } & \\text { N/A } \\\\\n",
    "\\hline \\text { SVD } & \\text { Fast } & \\text { No } & \\text { Slow } & 0 & \\text { No } & \\text { LinearRegression } \\\\\n",
    "\\hline \\text { Batch GD } & \\text { Slow } & \\text { No } & \\text { Fast } & 2 & \\text { Yes } & \\text { SGDRegressor } \\\\\n",
    "\\hline \\begin{array}{l}\n",
    "\\text { Stochastic } \\\\\n",
    "\\text { GD }\n",
    "\\end{array} & \\text { Fast } & \\text { Yes } & \\text { Fast } & \\geq 2 & \\text { Yes } & \\text { SGDRegressor } \\\\\n",
    "\\hline \\begin{array}{l}\n",
    "\\text { Mini-batch } \\\\\n",
    "\\text { GD }\n",
    "\\end{array} & \\text { Fast } & \\text { Yes } & \\text { Fast } & \\geq 2 & \\text { Yes } & \\text { SGDRegressor }\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Polynomial Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use linear regression models to train non linear data by adding combinations and higher degrees of features. This can be done with scikit-learn PloynomialFeatures class from <c>preprocessing</c> package. Note that when there are multiple features, Polynomial Regression is capable\n",
    "of finding relationships between features (which is something a plain Linear\n",
    "Regression model cannot do). This is made possible by the fact that\n",
    "PolynomialFeatures also adds all combinations of features up to the given\n",
    "degree. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
